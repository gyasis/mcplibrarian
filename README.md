# mcplibrarian

**One-command MCP server containerization â€” works across every AI coding assistant**

## The Problem

Every AI coding assistant (Cursor, VS Code Copilot, Windsurf, Goose, Continue.dev) loads all MCP tool definitions into context at startup. With 10+ servers, that's 30â€“100k tokens consumed before you type a single prompt.

Claude Code solved this natively in January 2026 with `defer_loading`. **Everyone else is still waiting.**

mcplibrarian gives Cursor, VS Code, Windsurf, and other platform users what Claude Code users now get natively â€” lazy-loaded, containerized MCP servers with a single command.

## Platform Support Matrix

| Platform | Native Lazy Loading | mcplibrarian Helps? |
|----------|--------------------|--------------------|
| Claude Code | âœ… Native (`defer_loading`) | For security & portability |
| **Cursor IDE** | âŒ None | **Yes â€” primary use case** |
| **VS Code Copilot** | âŒ None | **Yes â€” primary use case** |
| **Windsurf** | âŒ None | **Yes â€” primary use case** |
| **Continue.dev** | âŒ None (client-side) | **Yes â€” primary use case** |
| **Block's Goose** | Partial (own paradigm) | **Yes** |
| **opencode (SST)** | âŒ None (Issue #9350 open) | **Yes â€” primary use case** |
| **OpenAI Codex** | Alternative (Skills/AGENTS.md) | **Yes** |

## What It Does

- **Auto-detects** MCP server type (Python FastMCP, Node.js MCP SDK)
- **Generates** optimized Dockerfiles, `docker-compose.yml`, and README configs
- **Handles credentials securely** â€” no secrets baked into images, mounts via env_file
- **Handles edge cases** â€” paths with spaces, hardcoded paths in server code
- **Works with any platform** â€” outputs standard Docker configs, not platform-specific

## Installation

```bash
cd mcplibrarian
uv pip install -e .
```

## Usage

```bash
# Containerize an MCP server
mcplibrarian /path/to/mcp-server

# With build automation
mcplibrarian --output ./docker-configs --build /path/to/mcp-server
```

## Why Docker Containers?

Even on platforms with native lazy loading, containerized MCP servers provide:

1. **Security isolation** â€” servers can't access your host filesystem unless explicitly allowed
2. **Reproducibility** â€” same container works on any machine, any platform
3. **Team portability** â€” share one `docker-compose.yml` instead of per-developer setup
4. **Resource control** â€” stop idle containers, limit CPU/RAM per server

## Token Savings (Cursor, VS Code, Windsurf)

Using mcplibrarian with a Docker proxy pattern:

| Before | After | Savings |
|--------|-------|---------|
| All 10 servers loaded at startup (~50k tokens) | Proxy index only (~1.5k tokens) | ~97% |

## Part of a Broader Token Strategy

1. âœ… Disable unused MCP servers (~30k tokens saved)
2. âœ… Skills progressive disclosure (~37k tokens saved)
3. ğŸš§ Docker lazy loading (~13.5k tokens saved) â† This tool

## Project Structure

```
mcplibrarian/
â”œâ”€â”€ src/mcp_dockerize/
â”‚   â”œâ”€â”€ cli.py              # Main CLI
â”‚   â”œâ”€â”€ detectors/
â”‚   â”‚   â””â”€â”€ python.py       # Python FastMCP detector
â”‚   â”œâ”€â”€ generators/
â”‚   â”‚   â””â”€â”€ dockerfile.py   # Dockerfile generator
â”‚   â””â”€â”€ templates/          # Jinja2 templates (in progress)
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## Status

**Current:** Python FastMCP servers fully supported. Node.js support in progress.

See [progress tracker](memory-bank/progress.md) for full milestone status.

## License

MIT
